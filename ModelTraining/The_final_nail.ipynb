{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2edea4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# ... rest of your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7410f5d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97b315e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID_x DELIVERY_DATE DELIVERY_TIME  DELIVERY_MODE DELIVERY_STATUS  \\\n",
      "0   351     4/24/2021      18:30:00             30              \\N   \n",
      "1   351     4/24/2021      18:30:00             30              \\N   \n",
      "2   351     4/24/2021      18:30:00             30              \\N   \n",
      "3   351     4/24/2021      18:30:00             30              \\N   \n",
      "4   351     4/24/2021      18:30:00             30              \\N   \n",
      "\n",
      "   INVOICE_AMOUNT  TAXES  CATERING_COST  SHOW_PRICE  DISCOUNT  ...  \\\n",
      "0          1525.0    0.0         1525.0           9       0.0  ...   \n",
      "1          1525.0    0.0         1525.0           9       0.0  ...   \n",
      "2          1525.0    0.0         1525.0           9       0.0  ...   \n",
      "3          1525.0    0.0         1525.0           9       0.0  ...   \n",
      "4          1525.0    0.0         1525.0           9       0.0  ...   \n",
      "\n",
      "     CATEGORY_NAME  CATEGORY_ID     PRODUCT_NAME  UNIT PRICE_y IS_DAIRY_FREE  \\\n",
      "0     ENTREE - VEG          1.0        Dal Tadka  18.0   130.0           0.0   \n",
      "1  APPETIZER - VEG          1.0  Gobi Pepper Fry  18.0   140.0           0.0   \n",
      "2              NaN          NaN              NaN   NaN     NaN           NaN   \n",
      "3              NaN          NaN              NaN   NaN     NaN           NaN   \n",
      "4              NaN          NaN              NaN   NaN     NaN           NaN   \n",
      "\n",
      "  IS_GLUTEN_FREE IS_NUT_FREE IS_VEGAN  EXCLUDE_ITEM  \n",
      "0            1.0         1.0      0.0           0.0  \n",
      "1            0.0         1.0      0.0          -1.0  \n",
      "2            NaN         NaN      NaN           NaN  \n",
      "3            NaN         NaN      NaN           NaN  \n",
      "4            NaN         NaN      NaN           NaN  \n",
      "\n",
      "[5 rows x 47 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13084 entries, 0 to 13083\n",
      "Data columns (total 47 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   ID_x                13084 non-null  int64  \n",
      " 1   DELIVERY_DATE       13084 non-null  object \n",
      " 2   DELIVERY_TIME       13084 non-null  object \n",
      " 3   DELIVERY_MODE       13084 non-null  int64  \n",
      " 4   DELIVERY_STATUS     13084 non-null  object \n",
      " 5   INVOICE_AMOUNT      13084 non-null  float64\n",
      " 6   TAXES               13084 non-null  float64\n",
      " 7   CATERING_COST       13084 non-null  float64\n",
      " 8   SHOW_PRICE          13084 non-null  int64  \n",
      " 9   DISCOUNT            13084 non-null  float64\n",
      " 10  GUEST_COUNT         13084 non-null  int64  \n",
      " 11  PER_PERSON_RATE     13084 non-null  float64\n",
      " 12  LIVE_STALL_CHARGE   13084 non-null  float64\n",
      " 13  SETUP_CHARGE        13084 non-null  float64\n",
      " 14  EVENT_FACILITY_FEE  13084 non-null  object \n",
      " 15  SERVER_CHARGE       13084 non-null  object \n",
      " 16  GRATUITY            13084 non-null  object \n",
      " 17  CATERING_STATUS     13084 non-null  object \n",
      " 18  CATERING_LOCATION   13084 non-null  object \n",
      " 19  ID_y                13011 non-null  float64\n",
      " 20  CATERING_ID         13011 non-null  float64\n",
      " 21  CATERING_DET_ID     13011 non-null  float64\n",
      " 22  ITEM_DESC           13011 non-null  object \n",
      " 23  SPICE_LEVEL         10114 non-null  object \n",
      " 24  QUANTITY            13006 non-null  object \n",
      " 25  PRICE_x             13011 non-null  float64\n",
      " 26  DAIRY               13011 non-null  float64\n",
      " 27  GLUTEN              13011 non-null  float64\n",
      " 28  NUTS                13011 non-null  float64\n",
      " 29  VEGAN               13011 non-null  float64\n",
      " 30  COMMENTS            2030 non-null   object \n",
      " 31  LNL_LOCATION        13011 non-null  object \n",
      " 32  CREATED_DTTM        13011 non-null  object \n",
      " 33  CREATED_BY_ID       13011 non-null  object \n",
      " 34  UPDATED_DTTM        13011 non-null  object \n",
      " 35  UPDATED_BY_ID       13011 non-null  object \n",
      " 36  PRODUCT_ID          27 non-null     float64\n",
      " 37  CATEGORY_NAME       27 non-null     object \n",
      " 38  CATEGORY_ID         27 non-null     float64\n",
      " 39  PRODUCT_NAME        27 non-null     object \n",
      " 40  UNIT                27 non-null     float64\n",
      " 41  PRICE_y             27 non-null     float64\n",
      " 42  IS_DAIRY_FREE       27 non-null     float64\n",
      " 43  IS_GLUTEN_FREE      27 non-null     float64\n",
      " 44  IS_NUT_FREE         27 non-null     float64\n",
      " 45  IS_VEGAN            27 non-null     float64\n",
      " 46  EXCLUDE_ITEM        27 non-null     float64\n",
      "dtypes: float64(24), int64(4), object(19)\n",
      "memory usage: 4.7+ MB\n",
      "None\n",
      "               ID_x  DELIVERY_MODE  INVOICE_AMOUNT         TAXES  \\\n",
      "count  13084.000000   13084.000000    13084.000000  13084.000000   \n",
      "mean    1735.090798      31.018725     2256.838859     18.695935   \n",
      "std      619.283283       0.857340     5553.600754     92.182565   \n",
      "min      351.000000      30.000000        0.000000      0.000000   \n",
      "25%     1192.000000      30.000000      480.000000      0.000000   \n",
      "50%     1702.000000      31.000000      862.500000      0.000000   \n",
      "75%     2282.000000      32.000000     2250.000000      0.000000   \n",
      "max     2797.000000      32.000000   106610.000000   1533.400000   \n",
      "\n",
      "       CATERING_COST    SHOW_PRICE      DISCOUNT   GUEST_COUNT  \\\n",
      "count   13084.000000  13084.000000  13084.000000  13084.000000   \n",
      "mean     2351.176574      8.844925     57.436592     81.757184   \n",
      "std      5638.957806      0.361990    307.272728    112.399278   \n",
      "min         0.000000      8.000000      0.000000      1.000000   \n",
      "25%       500.000000      9.000000      0.000000     20.000000   \n",
      "50%       900.000000      9.000000      0.000000     50.000000   \n",
      "75%      2305.000000      9.000000      1.000000    100.000000   \n",
      "max    106610.000000      9.000000   5876.000000   1500.000000   \n",
      "\n",
      "       PER_PERSON_RATE  LIVE_STALL_CHARGE  ...         VEGAN  PRODUCT_ID  \\\n",
      "count     13084.000000       13084.000000  ...  13011.000000   27.000000   \n",
      "mean          3.559658          38.216906  ...     -0.525017  313.888889   \n",
      "std          13.094873         371.621382  ...      0.751151  178.604491   \n",
      "min           0.000000           0.000000  ...     -1.000000   13.000000   \n",
      "25%           0.000000           0.000000  ...     -1.000000  143.500000   \n",
      "50%           0.000000           0.000000  ...     -1.000000  344.000000   \n",
      "75%           0.000000           0.000000  ...      0.000000  489.000000   \n",
      "max         113.640000        7000.000000  ...      1.000000  517.000000   \n",
      "\n",
      "       CATEGORY_ID       UNIT     PRICE_y  IS_DAIRY_FREE  IS_GLUTEN_FREE  \\\n",
      "count         27.0  27.000000   27.000000      27.000000       27.000000   \n",
      "mean           1.0  21.962963   57.331852       0.296296        0.481481   \n",
      "std            0.0   6.066817   68.452130       0.465322        0.509175   \n",
      "min            1.0  18.000000    2.000000       0.000000        0.000000   \n",
      "25%            1.0  18.000000    3.000000       0.000000        0.000000   \n",
      "50%            1.0  19.000000    6.990000       0.000000        0.000000   \n",
      "75%            1.0  20.000000  140.000000       1.000000        1.000000   \n",
      "max            1.0  33.000000  160.000000       1.000000        1.000000   \n",
      "\n",
      "       IS_NUT_FREE   IS_VEGAN  EXCLUDE_ITEM  \n",
      "count    27.000000  27.000000     27.000000  \n",
      "mean      0.666667   0.296296      0.259259  \n",
      "std       0.480384   0.465322      0.525693  \n",
      "min       0.000000   0.000000     -1.000000  \n",
      "25%       0.000000   0.000000      0.000000  \n",
      "50%       1.000000   0.000000      0.000000  \n",
      "75%       1.000000   1.000000      1.000000  \n",
      "max       1.000000   1.000000      1.000000  \n",
      "\n",
      "[8 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = pd.read_csv('merged_catering_data.csv')\n",
    "\n",
    "\n",
    "print(data.head())\n",
    "print(data.info())\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4afdd070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values (replace with appropriate strategy)\n",
    "data.fillna(method='ffill', inplace=True)  # Replace with suitable imputation\n",
    "\n",
    "# Convert categorical features to numerical using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "data['CATEGORY_NAME'] = label_encoder.fit_transform(data['CATEGORY_NAME'])\n",
    "data['ITEM_DESC'] = label_encoder.fit_transform(data['ITEM_DESC'])\n",
    "\n",
    "# Feature engineering\n",
    "data['dish_category'] = data['CATEGORY_NAME']\n",
    "data['dish_type'] = data['ITEM_DESC']\n",
    "\n",
    "# Select relevant features\n",
    "features = ['GUEST_COUNT', 'dish_category', 'dish_type', 'PRICE_y', 'IS_DAIRY_FREE', 'IS_GLUTEN_FREE', 'IS_NUT_FREE', 'IS_VEGAN']\n",
    "X = data[features]\n",
    "y = data['QUANTITY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "553700a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7159be68",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e784ecda",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3046454205.py, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[10], line 10\u001b[1;36m\u001b[0m\n\u001b[1;33m    y_train = # ... your processed y_train ...\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "# Check for \"75 PIECES\" in y_train (modify as needed)\n",
    "if any(\"PIECES\" in str(val) for val in y_train):\n",
    "  # Handle the text data (e.g., remove rows, extract numeric part)\n",
    "  # ... your data cleaning logic here ...\n",
    "  y_train = # ... your processed y_train ...\n",
    "\n",
    "# Rest of your code (assuming y_train is now numerical)\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "# ... rest of your code\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print('Mean Squared Error:', mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39fba2d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input y contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 65\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Feature scaling (optional)\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# scaler = StandardScaler()\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# X_train = scaler.fit_transform(X_train)\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# X_test = scaler.transform(X_test)\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# Create and train the model\u001b[39;00m\n\u001b[0;32m     64\u001b[0m model \u001b[38;5;241m=\u001b[39m RandomForestRegressor(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m---> 65\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# Make predictions and evaluate\u001b[39;00m\n\u001b[0;32m     68\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:348\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(y):\n\u001b[0;32m    347\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse multilabel-indicator for y is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 348\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m    349\u001b[0m     X, y, multi_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mDTYPE\n\u001b[0;32m    350\u001b[0m )\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    352\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(sample_weight, X)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:621\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    619\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    620\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 621\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    622\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    624\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1163\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1143\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1144\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1145\u001b[0m     )\n\u001b[0;32m   1147\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m   1148\u001b[0m     X,\n\u001b[0;32m   1149\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1160\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1161\u001b[0m )\n\u001b[1;32m-> 1163\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m   1165\u001b[0m check_consistent_length(X, y)\n\u001b[0;32m   1167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1173\u001b[0m, in \u001b[0;36m_check_y\u001b[1;34m(y, multi_output, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1171\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Isolated part of check_X_y dedicated to y validation\"\"\"\u001b[39;00m\n\u001b[0;32m   1172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m multi_output:\n\u001b[1;32m-> 1173\u001b[0m     y \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m   1174\u001b[0m         y,\n\u001b[0;32m   1175\u001b[0m         accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1176\u001b[0m         force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   1177\u001b[0m         ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1178\u001b[0m         dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1179\u001b[0m         input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1180\u001b[0m         estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[0;32m   1181\u001b[0m     )\n\u001b[0;32m   1182\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1183\u001b[0m     estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:959\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    953\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    954\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    955\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    956\u001b[0m         )\n\u001b[0;32m    958\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m--> 959\u001b[0m         _assert_all_finite(\n\u001b[0;32m    960\u001b[0m             array,\n\u001b[0;32m    961\u001b[0m             input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m    962\u001b[0m             estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[0;32m    963\u001b[0m             allow_nan\u001b[38;5;241m=\u001b[39mforce_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    964\u001b[0m         )\n\u001b[0;32m    966\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    967\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:124\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 124\u001b[0m _assert_all_finite_element_wise(\n\u001b[0;32m    125\u001b[0m     X,\n\u001b[0;32m    126\u001b[0m     xp\u001b[38;5;241m=\u001b[39mxp,\n\u001b[0;32m    127\u001b[0m     allow_nan\u001b[38;5;241m=\u001b[39mallow_nan,\n\u001b[0;32m    128\u001b[0m     msg_dtype\u001b[38;5;241m=\u001b[39mmsg_dtype,\n\u001b[0;32m    129\u001b[0m     estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[0;32m    130\u001b[0m     input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m    131\u001b[0m )\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:173\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    159\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    161\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    172\u001b[0m     )\n\u001b[1;32m--> 173\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input y contains NaN."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def preprocess_data(data):\n",
    "  \"\"\"Preprocesses the data for model training.\n",
    "\n",
    "  Args:\n",
    "    data: The original pandas DataFrame.\n",
    "\n",
    "  Returns:\n",
    "    A tuple of (X, y) where X is the feature matrix and y is the target variable.\n",
    "  \"\"\"\n",
    "\n",
    "  # Handle missing values (replace with appropriate strategy)\n",
    "  data.fillna(method='ffill', inplace=True)\n",
    "\n",
    "  # Convert categorical features to numerical using LabelEncoder\n",
    "  label_encoder = LabelEncoder()\n",
    "  data['CATEGORY_NAME'] = label_encoder.fit_transform(data['CATEGORY_NAME'])\n",
    "  data['ITEM_DESC'] = label_encoder.fit_transform(data['ITEM_DESC'])\n",
    "\n",
    "  # Feature engineering\n",
    "  data['dish_category'] = data['CATEGORY_NAME']\n",
    "  data['dish_type'] = data['ITEM_DESC']\n",
    "\n",
    "  # Handle quantity column (assuming it's named 'QUANTITY')\n",
    "  data['QUANTITY'] = data['QUANTITY'].astype(str)  # Convert to string for cleaning\n",
    "  data['QUANTITY'] = data['QUANTITY'].str.replace(' PIECES', '', regex=False)  # Remove ' PIECES'\n",
    "  data['QUANTITY'] = pd.to_numeric(data['QUANTITY'], errors='coerce')  # Convert to numeric\n",
    "\n",
    "  # Handle missing values after quantity cleaning\n",
    "  data.fillna(method='ffill', inplace=True)\n",
    "\n",
    "  # Select relevant features\n",
    "  features = ['GUEST_COUNT', 'dish_category', 'dish_type', 'PRICE_y', 'IS_DAIRY_FREE', 'IS_GLUTEN_FREE', 'IS_NUT_FREE', 'IS_VEGAN']\n",
    "  X = data[features]\n",
    "  y = data['QUANTITY']\n",
    "\n",
    "  return X, y\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('merged_catering_data.csv')\n",
    "\n",
    "# Preprocess data\n",
    "# Check for NaN values in y (quantity)\n",
    "if y.isnull().sum() > 0:\n",
    "  # Handle NaN values (e.g., remove rows)\n",
    "  # ... your data cleaning logic here ...\n",
    "  data.dropna(subset=['QUANTITY'], inplace=True)  # Remove rows with NaN in QUANTITY\n",
    "  X, y = preprocess_data(data)  # Re-run preprocessing\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature scaling (optional)\n",
    "# scaler = StandardScaler()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_test = scaler.transform(X_test)\n",
    "\n",
    "# Create and train the model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print('Mean Squared Error:', mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d86ec0cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 41313.7013550034\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def preprocess_data(data):\n",
    "  \"\"\"Preprocesses the data for model training.\n",
    "\n",
    "  Args:\n",
    "    data: The original pandas DataFrame.\n",
    "\n",
    "  Returns:\n",
    "    A tuple of (X, y) where X is the feature matrix and y is the target variable.\n",
    "  \"\"\"\n",
    "\n",
    "  # Handle missing values (replace with appropriate strategy)\n",
    "  data.fillna(method='ffill', inplace=True)\n",
    "\n",
    "  # Convert categorical features to numerical using LabelEncoder\n",
    "  label_encoder = LabelEncoder()\n",
    "  data['CATEGORY_NAME'] = label_encoder.fit_transform(data['CATEGORY_NAME'])\n",
    "  data['ITEM_DESC'] = label_encoder.fit_transform(data['ITEM_DESC'])\n",
    "\n",
    "  # Feature engineering\n",
    "  data['dish_category'] = data['CATEGORY_NAME']\n",
    "  data['dish_type'] = data['ITEM_DESC']\n",
    "\n",
    "  # Handle quantity column (assuming it's named 'QUANTITY')\n",
    "  data['QUANTITY'] = data['QUANTITY'].astype(str)  # Convert to string for cleaning\n",
    "  data['QUANTITY'] = data['QUANTITY'].str.replace(' PIECES', '', regex=False)  # Remove ' PIECES'\n",
    "  data['QUANTITY'] = pd.to_numeric(data['QUANTITY'], errors='coerce')  # Convert to numeric\n",
    "\n",
    "  # Handle missing values after quantity cleaning\n",
    "  data.dropna(subset=['QUANTITY'], inplace=True)  # Remove rows with NaN in QUANTITY\n",
    "\n",
    "  # Select relevant features\n",
    "  features = ['GUEST_COUNT', 'dish_category', 'dish_type', 'PRICE_y', 'IS_DAIRY_FREE', 'IS_GLUTEN_FREE', 'IS_NUT_FREE', 'IS_VEGAN']\n",
    "  X = data[features]\n",
    "  y = data['QUANTITY']\n",
    "\n",
    "  # Impute missing values in y (if any)\n",
    "  imputer = SimpleImputer(strategy='mean')\n",
    "  y = imputer.fit_transform(y.values.reshape(-1, 1))[:, 0]\n",
    "\n",
    "  return X, y\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('merged_catering_data.csv')\n",
    "\n",
    "# Preprocess data\n",
    "X, y = preprocess_data(data)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature scaling (optional)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Create and train the model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print('Mean Squared Error:', mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4478fdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_instance = {\n",
    "    'GUEST_COUNT': 150,\n",
    "    'dish_category': 2,  \n",
    "    'dish_type': 5,     \n",
    "    'PRICE_y': 25.0,\n",
    "    'IS_DAIRY_FREE': 0,  \n",
    "    'IS_GLUTEN_FREE': 1,\n",
    "    'IS_NUT_FREE': 0,\n",
    "    'IS_VEGAN': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c87d24e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "sample_instance_array = np.array([list(sample_instance.values())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7bc567b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[150.   2.   5.  25.   0.   1.   0.   0.]]\n"
     ]
    }
   ],
   "source": [
    "print(sample_instance_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f41c3a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[49.21]\n"
     ]
    }
   ],
   "source": [
    "predicted_quantity = model.predict(sample_instance_array)\n",
    "print(predicted_quantity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a147122",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Assuming your trained model is named 'model'\n",
    "with open('model.pkl', 'wb') as file:\n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643b0698",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
